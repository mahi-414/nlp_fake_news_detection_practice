# -*- coding: utf-8 -*-
"""neural_net_w_feature_vector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dG0lKHg-aqDsAR2tHnSDBpa6yNHeazwV
"""

#import libraries
import pandas as pd
import numpy as np
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('wordnet')
lemmatizer = nltk.stem.WordNetLemmatizer()
nltk.download('punkt')
# from nltk.corpus import stopwords
# stop_words = set(stopwords.words('english'))
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

#upload data
data = pd.io.parsers.read_csv("politifact_data1csv.csv")
train = data.loc[:,['title','text','source','label']]

train = train.fillna(' ')
train['total'] = train['title'] + ' ' + train['text'] + ' ' + train['source']
print(train.shape)
print(train.head())

#word cloud
real_words = ''
fake_words = ''
stopwords = set(STOPWORDS)

for val in train[train['label'] == 1].total:
  tokens = val.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
  real_words += " ".join(tokens)+ " "

for val in train[train['label'] == 0].total:
  tokens = val.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
  fake_words += " ".join(tokens)+ " "


wordcloud = WordCloud(width = 800, height = 800,
                      background_color = 'white',
                      stopwords = stopwords,
                      min_font_size = 10).generate(fake_words)

plt.figure(figsize = (8,8), facecolor = None)
plt.imshow(wordcloud)
plt.tight_layout(pad = 0)
plt.show()

#cleaning data ***RUN TWICE***
for index,row in train.iterrows():
  filter_sentence = ''

  sentence = row['total']
  sentence = re.sub(r'[^\w\s]', '', sentence)
  
  words = nltk.word_tokenize(sentence)

  words = [word for word in words if not word in stopwords]

  for word in words:
    filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()

  train.loc[index, 'total'] = filter_sentence

print(train.loc[:, 'total'])

#transforming data
train = train[['total', 'label']]

X_train = train['total']
Y_train = train['label']

def vectorize_text(features, max_features):
  vectorizer = TfidfVectorizer(stop_words = 'english',
                               decode_error = 'strict',
                               analyzer = 'word',
                               ngram_range = (1, 2),
                               max_features = max_features)
  feature_vec = vectorizer.fit_transform(features)
  return feature_vec.toarray()

count_vectorizer = CountVectorizer()
count_vectorizer.fit_transform(X_train)
freq_term_matrix = count_vectorizer.transform(X_train)
tfidf = TfidfTransformer()
tfidf.fit(freq_term_matrix)
tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)
print(tf_idf_matrix.toarray().shape)

test_counts = count_vectorizer.transform(train['total'].values)
print(test_counts.shape)
test_tfidf = tfidf.transform(test_counts)

print(X_train.head())

#extracting features
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000) 

train_data_features = vectorizer.fit_transform(X_train)

train_data_features = train_data_features.toarray()
print(train_data_features.shape)

tfidf = TfidfTransformer()
tfidf_features = tfidf.fit_transform(train_data_features).toarray()
print(tfidf_features.shape)

vocab = vectorizer.get_feature_names()
# print(len(vocab))

bag_dictionary = pd.DataFrame()
bag_dictionary['ngram'] = vocab
bag_dictionary['count'] = train_data_features[1]
bag_dictionary['tfidf_features'] = tfidf_features[1]


bag_dictionary.sort_values(by=['count'], ascending=False, inplace=True)
# print(bag_dictionary.head(10))

#split into testing/training
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, Y_train, test_size=0.2)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#neural network MLP for binary classification
#ref: https://gist.github.com/candlewill/552fa102352ccce42fd829ae26277d24
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout

neurons_list = [600]
epochs_list = [50]
score_list = []

for neuron_num in neurons_list:
  for epoch_num in epochs_list:
    model = Sequential()
    model.add(Dense(neuron_num, input_dim=5000, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(neuron_num, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy']) #adam is best optimizer

    model.fit(X_train, y_train,
              epochs=epoch_num)
    score = model.evaluate(X_test, y_test)
    score_list.append("neurons: " + str(neuron_num) + ", epochs: " + str(epoch_num) + ", score: " + str(score))

for i in score_list:
  print(i)